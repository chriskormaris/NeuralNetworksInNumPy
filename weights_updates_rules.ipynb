{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X size: N x D+1 -> input data\n",
    "\n",
    "$W^{(1)}$ size: M x D+1 -> layer 1 weights\n",
    "\n",
    "$W^{(2)}$ size: K x M+1 -> layer 2 weights\n",
    "\n",
    "\n",
    "$S_1 = X \\cdot (W^{(1)})^T$ -> $S_1$ size: N x M\n",
    "\n",
    "$O_1$: activation_function_1($S_1$) -> $O_1$ size: N x M\n",
    "\n",
    "$O_1$ = [ones_column, $O_1$] -> $O_1$ size: N x M+1\n",
    "\n",
    "$S_2 = O_1 \\cdot (W^{(2)})^T$ -> $S_2$ size: N x K\n",
    "\n",
    "$O_2$ = activation_function_2($S_2$) -> $O_2$ size: N x K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Ascent Update Rules\n",
    "\n",
    "**where $Y = S_2$**\n",
    "\n",
    "Loss Function -> Mean Squared Error Loss:\n",
    "$$E(W) = \\frac{1}{2} \\cdot \\sum_{n=1}^{N}\\sum_{k=1}^{K}(T-Y)^2 - \\frac{λ}{2} \\cdot ||W||^2$$\n",
    "\n",
    "$$\\frac{\\partial E(W)}{\\partial W^{(2)}} = (T-Y)^T \\cdot O1 - \\frac{λ}{2} \\cdot W^{(2)}$$\n",
    "\n",
    "$$\\frac{\\partial E(W)}{\\partial W^{(1)}} = \\frac{\\partial E(W)}{\\partial S_2} \\cdot \\frac{\\partial S_2}{\\partial O_1} \\cdot \\frac{\\partial O_1}{\\partial S_1} \\cdot \\frac{\\partial S_1}{\\partial W^{(1)}} - \\frac{λ}{2} \\cdot W^{(1)} =>$$\n",
    "\n",
    "$$\\frac{\\partial E(W)}{\\partial W^{(1)}} = ( (T-Y) \\cdot W^{(2)}[:][2:end] \\cdot \\frac{\\partial O1}{\\partial S_1} )^T \\times X - \\frac{λ}{2} \\cdot W^{(1)}$$\n",
    "\n",
    "**where $\\times$ denotes the element-wise multiplication**\n",
    "\n",
    "The update rule for $W^{(1)}$ is: $$W^{(1)} = W^{(1)} + η \\cdot \\frac{\\partial E(W)}{\\partial W^{(1)}}$$\n",
    "\n",
    "The update rule for $W^{(2)}$ is: $$W^{(2)} = W^{(2)} + η \\cdot \\frac{\\partial E(W)}{\\partial W^{(2)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Update Rules\n",
    "\n",
    "**where $Y = S_2$**\n",
    "\n",
    "Loss Function -> Mean Squared Error Loss:\n",
    "$$E(W) = \\frac{1}{2} \\cdot \\sum_{n=1}^{N}\\sum_{k=1}^{K}(Y-T)^2 + \\frac{λ}{2} \\cdot ||W||^2$$\n",
    "\n",
    "$$\\frac{\\partial E(W)}{\\partial W^{(2)}} = (Y-T)^T \\cdot O1 + \\frac{λ}{2} \\cdot W^{(2)}$$\n",
    "\n",
    "$$\\frac{\\partial E(W)}{\\partial W^{(1)}} = \\frac{\\partial E(W)}{\\partial S_2} \\cdot \\frac{\\partial S_2}{\\partial O_1} \\cdot \\frac{\\partial O_1}{\\partial S_1} \\cdot \\frac{\\partial S_1}{\\partial W^(1)} + \\frac{λ}{2} \\cdot W^{(1)} =>$$\n",
    "\n",
    "$$\\frac{\\partial E(W)}{\\partial W^{(1)}} = ( (Y-T) \\cdot W^{(2)}[:][2:end] \\cdot \\frac{\\partial O_1}{\\partial S_1} )^T \\times X + \\frac{λ}{2} \\cdot W^{(1)}$$\n",
    "\n",
    "**where $\\times$ denotes the element-wise multiplication**\n",
    "\n",
    "The update rule for $W^{(1)}$ is: $$W^{(1)} = W^{(1)} - η \\cdot \\frac{\\partial E(W)}{\\partial W^{(1)}}$$\n",
    "\n",
    "The update rule for $W^{(2)}$ is: $$W^{(2)} = W^{(2)} - η \\cdot \\frac{\\partial E(W)}{\\partial W^{(2)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same update rules for Gradient Descent and Gradient Ascent apply in the case of Cross Entropy Loss Function incidentally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
